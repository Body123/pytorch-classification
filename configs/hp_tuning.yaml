search_space:
  weight_decay:
    _type: uniform
    _value: [0.00001, 0.01]
  learning_rate:
    _type: loguniform
    _value: [0.00001, 0.01]
  momentum:
    _type: uniform
    _value: [0.5, 1]
  batch_size:
    _type: choice
    _value: [16, 32]
  epochs:
    _type: choice
    _value: [20, 25, 30]
  warmup_epochs:
    _type: randint
    _value: [0, 6]
  optimizer:
    _type: choice
    _value: ['SGD', 'ADAM']

trial_command: python main.py -p -c ./configs/default.yaml
trial_code_directory: ..

trial_gpu_number: 1 # number of gpus to run a trial
trial_concurrency: 1 # number of trials to run experiments concurrently

# Set max_trial_number to limit number of trial or max_experiment_duration to limit running time.
# If neither max_trial_number nor max_experiment_duration are set, the experiment will run forever until you stop it.
max_trial_number: 100 # number of trials to run in total
# max_experiment_duration: 1h # max duration of the experiment.

tuner:
  name: TPE
  class_args:
    optimize_mode: maximize

training_service:
  platform: local
  use_active_gpu: true
  gpu_indices: 0 # specify gpu indices to use, e.g., 0,1,2,3